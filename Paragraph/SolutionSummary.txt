The solution I went for on ingesting logs in this particular challenge was using logstash stdin for my input plugin, a combination of grok and mutate for my filter plugins, and stdout and file for my output plugins.  Originally I was aiming to use file for my input plugin as well, but due to some file permission issues a quicker result was achieved via stdin.  I briefly explored beats for input and elasticsearch for output, but both those solutions would have required some significant creep of scope.  Once the log was being ingested via stdin, I tested out some parsing with a single match case in grok using the grok debugger, but some of the special characters were proving problamtic. Digging through the grok plugin filter page, I found both QUOTEDSTRING and break_on_match, which was the simplest way I could capture the fields without writing a significant amount of custom regex.  I used mutuate's update to transform the severity numbers to the text output required, with a series of if and else if statements.  Finally I used an extremely useful gsub mutation to strip out the excessive double quotes I was pulling into my fields from my QUOTEDSTRING grok pattern. I reviewed the output initially in the console via stdout, then checked the output file to insure logstash was writing to the file as expected.  Finally, I created several logs with altered data and ran them through logstash to ensure my filter could handle different data in the fields.